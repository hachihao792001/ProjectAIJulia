mutable struct NashQLearning
	ğ’« # Markov game
	i # agent index
	Q # state-action value estimates
	N # history of actions performed
end

function NashQLearning(ğ’«::MG, i)
	â„, ğ’®, ğ’œ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ
	Q = Dict((j, s, a) => 0.0 for j in â„, s in ğ’®, a in joint(ğ’œ))
	N = Dict((s, a) => 1.0 for s in ğ’®, a in joint(ğ’œ))
	return NashQLearning(ğ’«, i, Q, N)
end

function (Ï€i::NashQLearning)(s)
	ğ’«, i, Q, N = Ï€i.ğ’«, Ï€i.i, Ï€i.Q, Ï€i.N
	â„, ğ’®, ğ’œ, ğ’œi, Î³ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.ğ’œ[Ï€i.i], ğ’«.Î³
	M = NashEquilibrium()
	ğ’¢ = SimpleGame(Î³, â„, ğ’œ, a -> [Q[j, s, a] for j in â„])
	Ï€ = solve(M, ğ’¢)
	Ïµ = 1 / sum(N[s, a] for a in joint(ğ’œ))
	Ï€iâ€²(ai) = Ïµ/length(ğ’œi) + (1-Ïµ)*Ï€[i](ai)
	return SimpleGamePolicy(ai => Ï€iâ€²(ai) for ai in ğ’œi)
end

function update!(Ï€i::NashQLearning, s, a, sâ€²)
	ğ’«, â„, ğ’®, ğ’œ, R, Î³ = Ï€i.ğ’«, Ï€i.ğ’«.â„, Ï€i.ğ’«.ğ’®, Ï€i.ğ’«.ğ’œ, Ï€i.ğ’«.R, Ï€i.ğ’«.Î³
	i, Q, N = Ï€i.i, Ï€i.Q, Ï€i.N
	M = NashEquilibrium()
	ğ’¢ = SimpleGame(Î³, â„, ğ’œ, aâ€² -> [Q[j, sâ€², aâ€²] for j in â„])
	Ï€ = solve(M, ğ’¢)
	Ï€i.N[s, a] += 1
	Î± = 1 / sqrt(N[s, a])
	for j in â„
		Ï€i.Q[j,s,a] += Î±*(R(s,a)[j] + Î³*utility(ğ’¢,Ï€,j) - Q[j,s,a])
	end
end