
include("nashequilibrium.jl")
include("conditionalplan.jl")

struct POMGDynamicProgramming
    b # initial belief
    d # depth of conditional plans
end
#------------------------------------------
#------------Tá»I Æ¯U HÃ“A NashEquilibriuM----
#-------------Dynamic Programming----------
#------------------------------------------
#------------------------------------------
Pkg.add("JuMP")
using JuMP
    # HÃ m kiá»ƒm tra xem policy  Ï€i cÃ³ phá»¥ thuá»™c vÃ o má»™t policy khÃ¡c theo cÃ´ng thá»©c (... trong report)
function is_dominated(ğ’«::POMG, Î , i, Ï€i)
    â„, ğ’® = ğ’«.â„, ğ’«.ğ’®
    jointÎ noti = joint([Î [j] for j in â„ if j â‰  i])
    Ï€(Ï€iâ€², Ï€noti) = [j==i ? Ï€iâ€² : Ï€noti[j>i ? j-1 : j] for j in â„]
    Ui = Dict((Ï€iâ€², Ï€noti, s) => evaluate_plan(ğ’«, Ï€(Ï€iâ€², Ï€noti), s)[i]
                for Ï€iâ€² in Î [i], Ï€noti in jointÎ noti, s in ğ’®)
    model = Model(Ipopt.Optimizer)
    @variable(model, Î´)
    @variable(model, b[jointÎ noti, ğ’®] â‰¥ 0)
    @objective(model, Max, Î´)
    @constraint(model, [Ï€iâ€²=Î [i]],
        sum(b[Ï€noti, s] * (Ui[Ï€iâ€², Ï€noti, s] - Ui[Ï€i, Ï€noti, s])
        for Ï€noti in jointÎ noti for s in ğ’®) â‰¥ Î´)
    @constraint(model, sum(b) == 1)
    optimize!(model)
    return value(Î´) â‰¥ 0
end

using Random
    # Loáº¡i bá» cÃ¡c policy phá»¥ thuá»™c
function prune_dominated!(Î , ğ’«::POMG)
    done = false
    while !done
        done = true
        for i in shuffle(ğ’«.â„)
            for Ï€i in shuffle(Î [i])
                if length(Î [i]) > 1 && is_dominated(ğ’«, Î , i, Ï€i)
                    filter!(Ï€iâ€² -> Ï€iâ€² â‰  Ï€i, Î [i])
                    done = false
                    break
                end
            end
        end
    end
end

    # Loáº¡i bá» khÃ¡c policy phá»¥ thuá»™c, chuyá»ƒn bÃ i toÃ¡n vá» SimpleGame vÃ  thá»±c hiá»‡n NashEquilibrium
function solveDP(M::POMGDynamicProgramming, ğ’«::POMG)
    â„, ğ’®, ğ’œ, R, Î³, b, d = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R, ğ’«.Î³, M.b, M.d
    # Táº¡o ConditionalPlan
    Î  = [[ConditionalPlan(ai) for ai in ğ’œ[i]] for i in â„]
    # XÃ©t theo Ä‘á»™ sÃ¢u d cá»§a plan
    for t in 1:d
        # Má»Ÿ rá»™ng plan theo Ä‘á»™ sÃ¢u, náº¿u cÃ³ sá»± phá»¥ thuá»™c thÃ¬ loáº¡i bá»
        Î  = expand_conditional_plans(ğ’«, Î )
        prune_dominated!(Î , ğ’«)
    end

    # Chuyá»ƒn vá» dáº¡ng simple game
    ğ’¢ = SimpleGame(Î³, â„, Î , Ï€ -> utility(ğ’«, b, Ï€))
    Ï€ = solveNE(NashEquilibrium(), ğ’¢)
    return Tuple(argmax(Ï€i.p) for Ï€i in Ï€)
end
    
struct SimpleGame
    Î³ # discount factor
    â„ # agents
    ğ’œ # joint action space
    R # joint reward function
end
    

struct NashEquilibrium end

function tensorform(ğ’«::SimpleGame)
    â„, ğ’œ, R = ğ’«.â„, ğ’«.ğ’œ, ğ’«.R
    â„â€² = eachindex(â„)
    ğ’œâ€² = [eachindex(ğ’œ[i]) for i in â„]
    Râ€² = [R(a) for a in joint(ğ’œ)]
    return â„â€², ğ’œâ€², Râ€²
end

function solveNE(M::NashEquilibrium, ğ’«::SimpleGame)
    â„, ğ’œ, R = tensorform(ğ’«)
    model = Model(Ipopt.Optimizer)
    @variable(model, U[â„])
    @variable(model, Ï€[i=â„, ğ’œ[i]] â‰¥ 0)
    @NLobjective(model, Min,
        sum(U[i] - sum(prod(Ï€[j,a[j]] for j in â„) * R[y][i]
            for (y,a) in enumerate(joint(ğ’œ))) for i in â„))
    @NLconstraint(model, [i=â„, ai=ğ’œ[i]],
        U[i] â‰¥ sum(
            prod(j==i ? (a[j]==ai ? 1.0 : 0.0) : Ï€[j,a[j]] for j in â„)
            * R[y][i] for (y,a) in enumerate(joint(ğ’œ))))
    @constraint(model, [i=â„], sum(Ï€[i,ai] for ai in ğ’œ[i]) == 1)
    optimize!(model)
    Ï€iâ€²(i) = SimpleGamePolicy(ğ’«.ğ’œ[i][ai] => value(Ï€[i,ai]) for ai in ğ’œ[i])
    return [Ï€iâ€²(i) for i in â„]
end

