import Pkg
using LinearAlgebra
using Distributions
using Plots

struct SimpleGame
      Î³  # discount factor
      â„  # agents
      ğ’œ  # joint action space
      R  # joint reward function
end

struct Travelers end

n_agents(simpleGame::Travelers) = 2

ordered_actions(simpleGame::Travelers, i::Int) = 2:100
ordered_joint_actions(simpleGame::Travelers) = vec(collect(Iterators.product([ordered_actions(simpleGame, i) for i in 1:n_agents(simpleGame)]...)))

n_joint_actions(simpleGame::Travelers) = length(ordered_joint_actions(simpleGame))
n_actions(simpleGame::Travelers, i::Int) = length(ordered_actions(simpleGame, i))

struct SetCategorical{S}
      elements::Vector{S} # Set elements (could be repeated)
      distr::Categorical # Categorical distribution over set elements

      function SetCategorical(elements::AbstractVector{S}) where {S}
            weights = ones(length(elements))
            return new{S}(elements, Categorical(normalize(weights, 1)))
      end

      function SetCategorical(elements::AbstractVector{S}, weights::AbstractVector{Float64}) where {S}
            â„“â‚ = norm(weights, 1)
            if â„“â‚ < 1e-6 || isinf(â„“â‚)
                  return SetCategorical(elements)
            end
            distr = Categorical(normalize(weights, 1))
            return new{S}(elements, distr)
      end
end


function reward(simpleGame::Travelers, i::Int, a)
      if i == 1
            noti = 2
      else
            noti = 1
      end
      if a[i] == a[noti]
            r = a[i]
      elseif a[i] < a[noti]
            r = a[i] + 2
      else
            r = a[noti] - 1
      end
      return r
end

function joint_reward(simpleGame::Travelers, a)
      return [reward(simpleGame, i, a) for i in 1:n_agents(simpleGame)]
end

function SimpleGame(simpleGame::Travelers)
      return SimpleGame(
            0.9,
            vec(collect(1:n_agents(simpleGame))),
            [ordered_actions(simpleGame, i) for i in 1:n_agents(simpleGame)],
            (a) -> joint_reward(simpleGame, a)
      )
end

struct SimpleGamePolicy
      p # dictionary mapping actions to probabilities
      function SimpleGamePolicy(p::Base.Generator)
            return SimpleGamePolicy(Dict(p))
      end
      function SimpleGamePolicy(p::Dict)
            vs = collect(values(p))
            vs ./= sum(vs)
            return new(Dict(k => v for (k, v) in zip(keys(p), vs)))
      end
      SimpleGamePolicy(ai) = new(Dict(ai => 1.0))
end

(Ï€i::SimpleGamePolicy)(ai) = get(Ï€i.p, ai, 0.0)
function (Ï€i::SimpleGamePolicy)()
      D = SetCategorical(collect(keys(Ï€i.p)), collect(values(Ï€i.p)))
      return rand(D)
end

joint(X) = vec(collect(Iterators.product(X...)))
joint(Ï€, Ï€i, i) = [i == j ? Ï€i : Ï€j for (j, Ï€j) in enumerate(Ï€)]

function utility(ğ’«::SimpleGame, Ï€, i)
      ğ’œ, R = ğ’«.ğ’œ, ğ’«.R
      p(a) = prod(Ï€j(aj) for (Ï€j, aj) in zip(Ï€, a))
      return sum(R(a)[i] * p(a) for a in joint(ğ’œ))
end



# ----------------------------- IteratedBestResponse --------------------------------
function best_response(ğ’«::SimpleGame, Ï€, i)
      U(ai) = utility(ğ’«, joint(Ï€, SimpleGamePolicy(ai), i), i)
      ai = argmax(U, ğ’«.ğ’œ[i])
      return SimpleGamePolicy(ai)
end

struct IteratedBestResponse
      k_max # number of iterations
      Ï€ # initial policy
end
function IteratedBestResponse(ğ’«::SimpleGame, k_max)
      Ï€ = [SimpleGamePolicy(ai => 1.0 for ai in ğ’œi) for ğ’œi in ğ’«.ğ’œ]
      return IteratedBestResponse(k_max, Ï€)
end
function solve(M::IteratedBestResponse, ğ’«)
      Ï€ = M.Ï€
      for k in 1:M.k_max
            Ï€ = [best_response(ğ’«, Ï€, i) for i in ğ’«.â„]
      end
      return Ï€
end

# ----------------------------- HierarchicalSoftmax --------------------------------
function softmax_response(ğ’«::SimpleGame, Ï€, i, Î»)
      ğ’œi = ğ’«.ğ’œ[i]
      U(ai) = utility(ğ’«, joint(Ï€, SimpleGamePolicy(ai), i), i)
      return SimpleGamePolicy(ai => exp(Î» * U(ai)) for ai in ğ’œi)
end

struct HierarchicalSoftmax
      Î» # precision parameter
      k # level
      Ï€ # initial policy
end
function HierarchicalSoftmax(ğ’«::SimpleGame, Î», k)
      Ï€ = [SimpleGamePolicy(ai => 1.0 for ai in ğ’œi) for ğ’œi in ğ’«.ğ’œ]
      return HierarchicalSoftmax(Î», k, Ï€)
end
function solve(M::HierarchicalSoftmax, ğ’«)
      Ï€ = M.Ï€
      for k in 1:M.k
            Ï€ = [softmax_response(ğ’«, Ï€, i, M.Î») for i in ğ’«.â„]
      end
      return Ï€
end

simpleGame = Travelers()
p = SimpleGame(simpleGame)

HS = HierarchicalSoftmax(p, 0.5, 10)
IBR = IteratedBestResponse(p, 100)

print("Begin solving IBR...")
Ï€ = solve(IBR, p)
print("\nNash equilibrium: ")
print(keys(Ï€[1].p), keys(Ï€[2].p))

print("Begin solving HS")
D = solve(HS, p)
bar(collect(keys(D[1].p)), collect(values(D[1].p)), orientation=:vertical)